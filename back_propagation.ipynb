{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Это код для создания и обучения двуслойной нейронной сети для задачи классификации, используя метод обратного распространения ошибки (backpropagation).\n",
    "\n",
    "Функция `f(x)` - это функция активации (в данном случае используется гиперболический тангенс), которая будет использоваться для вычисления выходного сигнала каждого нейрона в сети.\n",
    "\n",
    "Функция `df(x)` - это производная функции активации, необходимая для вычисления локального градиента при использовании метода обратного распространения ошибки.\n",
    "\n",
    "Матрицы `W1` и `W2` представляют собой весовые коэффициенты сети. `W1` - это веса между входным слоем и скрытым слоем, а `W2` - это веса между скрытым слоем и выходным слоем.\n",
    "\n",
    "Функция `go_forward(inp)` - это функция прямого прохода, которая принимает входной сигнал `inp` и возвращает выходной сигнал сети.\n",
    "\n",
    "Функция `train(epoch)` - это функция обучения, которая использует обратное распространение для корректировки весов сети на каждой итерации. В цикле `for` происходит случайный выбор входного сигнала из обучающей выборки, прямой проход по НС и вычисление выходных значений нейронов. Затем с помощью локального градиента вычисляется ошибка и корректируются веса связей между нейронами.\n",
    "\n",
    "Обучающая выборка содержит набор примеров, на основе которых сеть будет обучаться.\n",
    "\n",
    "В конце скрипта происходит проверка работоспособности сети на каждом примере обучающей выборки.\n",
    "\n",
    "Общий алгоритм заключается в многократном обучении сети на обучающей выборке до тех пор, пока градиент ошибки на всех примерах не станет меньше значения, заданного на пороге. Когда сеть была обучена, она может быть использована для решения задачи классификации на новых данных.\n",
    "\n",
    "Как уже было сказано, представленный код решает задачу классификации, где нужно разделить данные на два класса: 1 и -1. Обучающая выборка содержит значения входных признаков и соответствующие им классы. Всего выборка содержит 8 примеров.\n",
    "\n",
    "В функции `f(x)` используется гиперболический тангенс, который может принимать значения от -1 до 1. Преимущество использования именно этой функции активации заключается в том, что она очень схожа с функцией логистической регрессии. Кроме того, гиперболический тангенс более подходит для решения задач классификации, чем сигмоидальная функция, которая часто используется в нейронных сетях.\n",
    "\n",
    "В функции `train(epoch)` используется метод обратного распространения ошибки (backpropagation). Этот метод заключается в том, что на каждой итерации обучения происходит прямой проход по сети, затем с помощью основанного на дифференцировании градиента вычисляется ошибка и корректируются веса связей между нейронами.\n",
    "\n",
    "Цикл `for` внутри функции `train(epoch)` проходит через выборку `epoch` и на каждой итерации выбирается случайный пример. `go_forward(x[0:3])` вызывает функцию для прямого прохода по нейронной сети. `y` - это выходной сигнал сети, а `out` - это выходы скрытого слоя. Затем считается ошибка `e=y-x[-1]`, где `x[-1]` - это правильный ответ. Вектор из двух величин локальных градиентов `delta2` вычисляется как произведение матрицы весов `W2` на вектор, состоящий из ошибки `delta` и локального градиента `df(out)`. Далее веса корректируются с помощью вычитания произведения локального градиента на входные значения нейронов (в случае `W1`) и выходные значения скрытого слоя (в случае `W2`).\n",
    "\n",
    "После того, как нейронная сеть была обучена на обучающей выборке, происходит проверка работы сети на каждом примере обучающей выборки. Нейронная сеть выдает предсказанные значения. Если они совпадают с правильными, то нейронная сеть дает правильный ответ.\n",
    "\n",
    "Такой подход к решению задачи классификации довольно стандартен и может быть использован для обучения других архитектур нейронных сетей. Правда, стоит заметить, что в некоторых случаях может потребоваться использование более сложных сетей или других методов обучения."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выходное значение НС: 0.09757375061638385 => -1\n",
      "Выходное значение НС: 0.9379945382203221 => 1\n",
      "Выходное значение НС: -0.87185122260772 => -1\n",
      "Выходное значение НС: 0.8784269878046129 => 1\n",
      "Выходное значение НС: -0.8784269878046129 => -1\n",
      "Выходное значение НС: 0.8718512226077197 => 1\n",
      "Выходное значение НС: -0.937994538220322 => -1\n",
      "Выходное значение НС: -0.09757375061638407 => -1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return 2/(1 + np.exp(-x)) - 1\n",
    "\n",
    "def df(x):\n",
    "    return 0.5*(1 + x)*(1 - x)\n",
    "\n",
    "W1 = np.array([[-0.2, 0.3, -0.4], [0.1, -0.3, -0.4]])\n",
    "W2 = np.array([0.2, 0.3])\n",
    "\n",
    "def go_forward(inp):\n",
    "    sum = np.dot(W1, inp)\n",
    "    out = np.array([f(x) for x in sum])\n",
    "\n",
    "    sum = np.dot(W2, out)\n",
    "    y = f(sum)\n",
    "    return y, out\n",
    "\n",
    "def train(epoch):\n",
    "    global W2, W1\n",
    "    lmd = 0.01          # шаг обучения\n",
    "    N = 10000           # число итераций при обучении\n",
    "    count = len(epoch)\n",
    "    for k in range(N):\n",
    "        x = epoch[np.random.randint(0, count)]  # случайных выбор входного сигнала из обучающей выборки\n",
    "        y, out = go_forward(x[0:3])             # прямой проход по НС и вычисление выходных значений нейронов\n",
    "        e = y - x[-1]                           # ошибка\n",
    "        delta = e*df(y)                         # локальный градиент\n",
    "        W2[0] = W2[0] - lmd * delta * out[0]    # корректировка веса первой связи\n",
    "        W2[1] = W2[1] - lmd * delta * out[1]    # корректировка веса второй связи\n",
    "\n",
    "        delta2 = W2*delta*df(out)               # вектор из 2-х величин локальных градиентов\n",
    "\n",
    "        # корректировка связей первого слоя\n",
    "        W1[0, :] = W1[0, :] - np.array(x[0:3]) * delta2[0] * lmd\n",
    "        W1[1, :] = W1[1, :] - np.array(x[0:3]) * delta2[1] * lmd\n",
    "\n",
    "# обучающая выборка (она же полная выборка)\n",
    "epoch = [(-1, -1, -1, -1),\n",
    "         (-1, -1, 1, 1),\n",
    "         (-1, 1, -1, -1),\n",
    "         (-1, 1, 1, 1),\n",
    "         (1, -1, -1, -1),\n",
    "         (1, -1, 1, 1),\n",
    "         (1, 1, -1, -1),\n",
    "         (1, 1, 1, -1)]\n",
    "\n",
    "train(epoch)        # запуск обучения сети\n",
    "\n",
    "# проверка полученных результатов\n",
    "for x in epoch:\n",
    "    y, out = go_forward(x[0:3])\n",
    "    print(f\"Выходное значение НС: {y} => {x[-1]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
