{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Этот код реализует обучение трёхслойной нейронной сети методом обратного распространения ошибки (Back Propagation) на задаче бинарной классификации.\n",
    "\n",
    "### Код позволяет обучить трёхслойную нейронную сеть методом обратного распространения ошибки на задаче бинарной классификации, и получить выходное значение для набора тестовых данных, придельно близкое к правильному ответу.\n",
    "\n",
    "### 1. Определяются функции активации f(x) и ее производная df(x). В данном случае используется логистическая функция:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# add activation function\n",
    "# logistic function\n",
    "$$f(x) = \\dfrac{1}{1+e^{-x}}$$\n",
    "\n",
    "# Функция $df(x) = 0.5 \\cdot (1+x) \\cdot (1-x)$ является производной от логистической функции описанной выше"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2/(1 + np.exp(-x)) - 1\n",
    "\n",
    "def df(x):\n",
    "    return 0.5*(1 + x)*(1 - x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "### 2. Задаются веса связей между нейронами входного, скрытого и выходного слоёв:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "W1 = np.array([[-0.2, 0.3, -0.4], [0.1, -0.3, -0.4]])\n",
    "W2 = np.array([0.2, 0.3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "### 3. Функция go_forward(inp) осуществляет прямой проход (тоесть распространения вперед) по сети и вычисляет выходное значение:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def go_forward(inp): # пропускаем вектор наблюдений через NN\n",
    "    sum = np.dot(W1, inp)\n",
    "    out = np.array([f(x) for x in sum]) # для каждого нейрона запомним выходные значения\n",
    "\n",
    "    sum = np.dot(W2, out)\n",
    "    y = f(sum) # выход из NN\n",
    "    return y, out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Функция train(epoch) реализует обратное распространение ошибки:\n",
    "   * Последовательно прогоняет обучающую выборку через сеть, получает выходные значения и вычисляет ошибку (ошибка — это разница между правильным ответом и предсказанным ответом).\n",
    "   * К выходному слою применяется локальный градиент, на основе которого обновляются веса входных связей скрытого и выходного слоев сети."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    global W2, W1\n",
    "    lmd = 0.01          # шаг обучения\n",
    "    N = 10000           # число итераций при обучении\n",
    "    count = len(epoch)\n",
    "    for k in range(N):\n",
    "        x = epoch[np.random.randint(0, count)]  # случайный выбор входного сигнала из обучающей выборки\n",
    "        y, out = go_forward(x[0:3]) # прямой проход по нейронной сети и вычисление выходных значений\n",
    "        e = y - x[-1] # вычисление ошибки\n",
    "        delta = e*df(y) # локальный градиент\n",
    "        W2[0] = W2[0] - lmd * delta * out[0] # корректировка веса первой связи\n",
    "        W2[1] = W2[1] - lmd * delta * out[1] # корректировка веса второй связи\n",
    "\n",
    "        delta2 = W2*delta*df(out)# вектор из 2-х величин локальных градиентов\n",
    "\n",
    "        # корректировка связей первого слоя\n",
    "        W1[0, :] = W1[0, :] - np.array(x[0:3]) * delta2[0] * lmd\n",
    "        W1[1, :] = W1[1, :] - np.array(x[0:3]) * delta2[1] * lmd\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Обучающая выборка epoch содержит четыре признака и один выходной параметр (-1 или 1). Функция train(epoch) запускается для N=10000 итераций для случайно выбранных входных параметров из epoch(эпох)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "epoch = [(-1, -1, -1, -1),\n",
    "         (-1, -1, 1, 1),\n",
    "         (-1, 1, -1, -1),\n",
    "         (-1, 1, 1, 1),\n",
    "         (1, -1, -1, -1),\n",
    "         (1, -1, 1, 1),\n",
    "         (1, 1, -1, -1),\n",
    "         (1, 1, 1, -1)]\n",
    "\n",
    "train(epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}